{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_GXjHffmz_Id"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.nn.utils import clip_grad_norm_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM = Long-Short-Term Memory is a type of recurrent neural network (RNN).\n",
        "\n",
        "embed_size = 128 # The numbner of values we are using to represent a word. This also represents the number of input features to LSTM.\n",
        "hidden_size = 1024 # The number of hidden units.\n",
        "num_layers = 2 # The number of layers.\n",
        "num_epochs = 20 # The number of epochs / complete iterations through a given dataset.\n",
        "words_batch_size = 20 # The number of training samples / words.\n",
        "time_steps = 16 # The number of previous words that will be looked at.\n",
        "learning_rate = 0.002 # The learning rate.\n",
        "num_words = 160 # The number of new words we want to generate.\n",
        "\n",
        "file_path = 'luceafarul.txt'"
      ],
      "metadata": {
        "id": "yWbd7DC9vAnT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to capture all the words from the poem, into a dictionary.\n",
        "class WordDictionary(object):\n",
        "  def __init__(self):\n",
        "    self.word_to_idx = {}\n",
        "    self.idx_to_word = {}\n",
        "    self.idx = 0\n",
        "\n",
        "  def add_word(self, word):\n",
        "    # Check if the word is not already in the dictionary.\n",
        "    if word not in self.word_to_idx:\n",
        "      self.word_to_idx[word] = self.idx # Map the word to an index.\n",
        "      self.idx_to_word[self.idx] = word # Map the index to a word.\n",
        "      self.idx = self.idx + 1\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.word_to_idx)"
      ],
      "metadata": {
        "id": "rZntzM5yjkoF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to process the words from file.\n",
        "class WordProcess(object):\n",
        "  def __init__(self):\n",
        "    self.dictionary = WordDictionary()\n",
        "\n",
        "  # Parse the file and capture all the words.\n",
        "  def get_data(self, path, batch_size):\n",
        "    with open(path, 'r') as f:\n",
        "      tokens = 0\n",
        "\n",
        "      # Loop through each line of the file.\n",
        "      for line in f:\n",
        "        words = line.split() # Split each line and create a list of words.\n",
        "        tokens = tokens + len(words) # Increment the tokens by the total number of words from each line.\n",
        "        \n",
        "        # Loop through the list of words.\n",
        "        for word in words:\n",
        "          self.dictionary.add_word(word) # Add each word into the word dictionary.\n",
        "\n",
        "    # Create a 1D tensor that captures all the words indexes.\n",
        "    idx_tensor = torch.LongTensor(tokens)\n",
        "    index = 0\n",
        "\n",
        "    with open(path, 'r') as f:\n",
        "      for line in f:\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "          idx_tensor[index] = self.dictionary.word_to_idx[word] # Map the tensor index to word index.\n",
        "          index = index + 1\n",
        "  \n",
        "    num_batches = idx_tensor.shape[0] // batch_size # We use // to ignore the remainder when we devide.\n",
        "    idx_tensor = idx_tensor[:num_batches * batch_size] # Trim the tensor to fit the batch size of 20.\n",
        "    idx_tensor = idx_tensor.view(batch_size, -1) # Reshape the tensor to have the number of rows 20 (batch size).\n",
        "    return idx_tensor"
      ],
      "metadata": {
        "id": "2uvrtB5Tjkqg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = WordProcess()\n",
        "idx_tensor = data.get_data(file_path, words_batch_size) # Load the file content into tensor.\n",
        "print (idx_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teRML5qcu-7r",
        "outputId": "9211a89c-87a0-4ad4-f1ad-ec6852322b5d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 88])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(data.dictionary)\n",
        "print (\"We have {} unique words.\".format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQxvbgXb0AaI",
        "outputId": "d63c3037-2c28-47f7-c7a1-899c5d88d5f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 982 unique words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = idx_tensor.shape[1] // time_steps # The number of batches required to complete the dataset (112 / 16).\n",
        "print (num_batches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtGmqXoe021-",
        "outputId": "ffde8a49-6a55-4ab4-81ba-62c5dc7bc92c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class that will generate the new text.\n",
        "class GenerateText(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "    super(GenerateText, self).__init__() # Use super function to inherit from nn.Module.\n",
        "    self.embed = nn.Embedding(vocab_size, embed_size) # Create the embedding layer that maps the words to features.\n",
        "    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first = True) # Create the LSTM layer.\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size) # Create the linear layer.\n",
        "\n",
        "  # Propagate the data through the neural network (x = inputs, h = hidden states).\n",
        "  def forward(self, x, h):\n",
        "    x = self.embed(x)\n",
        "    out, (h, c) = self.lstm(x, h)\n",
        "    out = out.reshape(out.size(0) * out.size(1), out.size(2))\n",
        "    out = self.linear(out)\n",
        "    return out, (h, c)"
      ],
      "metadata": {
        "id": "6-9gfevCu--8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenerateText(vocab_size, embed_size, hidden_size, num_layers) # Define the model.\n",
        "loss_func = nn.CrossEntropyLoss() # Define the loss function.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) # Define the optimization algorithm."
      ],
      "metadata": {
        "id": "lxgaOpK85fMe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using the mini-batch gradient descent.\n",
        "for epoch in range(num_epochs):\n",
        "  # Initialize the cell and hidden states.\n",
        "  states = (torch.zeros(num_layers, words_batch_size, hidden_size),\n",
        "            torch.zeros(num_layers, words_batch_size, hidden_size))\n",
        "  \n",
        "  # Iterate over the training dataset.\n",
        "  for i in range(0, idx_tensor.size(1) - time_steps, time_steps):\n",
        "    inputs = idx_tensor[:, i:i + time_steps]\n",
        "    targets = idx_tensor[:, (i + 1):(i + 1) + time_steps]\n",
        "    outputs,_ = model(inputs, states) # Feed the inputs and the cell & hidden states into the RNN model. \n",
        "    loss = loss_func(outputs, targets.reshape(-1)) # Calculate the loss between the predicted outputs and targets.\n",
        "\n",
        "    # Perform back-propagation and weight updates.\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    clip_grad_norm_(model.parameters(), 0.5) # Clip the gradient to avoid the exploding gradient problem.\n",
        "    optimizer.step() # Update the model parameters, using the step method.\n",
        "\n",
        "    step = (i + 1) // time_steps\n",
        "    \n",
        "    if step % 100 == 0:\n",
        "      print ('Epoch: {}/{}, Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NetV4C_Djksx",
        "outputId": "db8c3d0d-a2eb-4834-9e7f-e2fd730511dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20, Loss: 6.8896\n",
            "Epoch: 2/20, Loss: 6.4778\n",
            "Epoch: 3/20, Loss: 6.3274\n",
            "Epoch: 4/20, Loss: 5.8894\n",
            "Epoch: 5/20, Loss: 5.0546\n",
            "Epoch: 6/20, Loss: 4.2351\n",
            "Epoch: 7/20, Loss: 3.4818\n",
            "Epoch: 8/20, Loss: 2.6834\n",
            "Epoch: 9/20, Loss: 2.0570\n",
            "Epoch: 10/20, Loss: 1.6266\n",
            "Epoch: 11/20, Loss: 1.3094\n",
            "Epoch: 12/20, Loss: 1.0690\n",
            "Epoch: 13/20, Loss: 0.9256\n",
            "Epoch: 14/20, Loss: 0.6607\n",
            "Epoch: 15/20, Loss: 0.5286\n",
            "Epoch: 16/20, Loss: 0.4334\n",
            "Epoch: 17/20, Loss: 0.3063\n",
            "Epoch: 18/20, Loss: 0.2202\n",
            "Epoch: 19/20, Loss: 0.1016\n",
            "Epoch: 20/20, Loss: 0.0590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model.\n",
        "words_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Initialize the cell and hidden states.\n",
        "  state = (torch.zeros(num_layers, 1, hidden_size),\n",
        "           torch.zeros(num_layers, 1, hidden_size))\n",
        "  \n",
        "  input = torch.randint(0, vocab_size, (1,)).long().unsqueeze(1) # Randomly select one word id an convert it to shape (1, 1)\n",
        "\n",
        "  for i in range(num_words):\n",
        "    output,_ = model(input, state) # Feed the input and the cell & hidden states into the RNN model. \n",
        "\n",
        "    sample_id = output.exp() # Sample a word id from the exponential output. \n",
        "    word_id = torch.multinomial(sample_id, num_samples = 1).item()\n",
        "    input.fill_(word_id) # Replace the input with sampled word id for the next time step.\n",
        "\n",
        "    word = data.dictionary.idx_to_word[word_id]\n",
        "    word = word + ' '\n",
        "    \n",
        "    words_list.append(word) # Append the new word to the list of words."
      ],
      "metadata": {
        "id": "gA7uWZigPDGY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to create the usual poem format (4 words x 4 lines).\n",
        "def print_new_words(w_list):\n",
        "  for i, w in enumerate(w_list):\n",
        "    if i % 4 == 0 and i != 0:\n",
        "      print('\\n', end = '') # Add a new line every 4 words.\n",
        "      if i % 16 == 0:\n",
        "        print ('\\n', end = '') # Add an empty row, every 4 lines.\n",
        "\n",
        "    print (w + ' ', end = '')"
      ],
      "metadata": {
        "id": "7ZigDteESBE0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_new_words(words_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2LwBDT1VyDg",
        "outputId": "4ea9fd95-c6fd-43c8-f761-67a70fc6f0c5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Un  Pe  marginea  pas  \n",
            "-  cei  casă  Lucesc  \n",
            "paşte,  nu-nţelegi  el  asculta  \n",
            "tău  tâmple,  faţa,  Pe-a  \n",
            "\n",
            "spre  a  se  culce  \n",
            "împărăteşti,  să  dau  tu  \n",
            "izvor  de  săptămâni,  îl  \n",
            "Uimirea-n  senine.  vom  E  \n",
            "\n",
            "S-a  curmă,  cercuri  lung  \n",
            "de  moarte.  tău  pânditor  \n",
            "în  gând,  pierde  dragu-unei  \n",
            "Cătălina.  Mai  mândră  păcat,  \n",
            "\n",
            "iarăşi  se  glas  ochii  \n",
            "mea  de-o  bălaie.  S-a  \n",
            "Ca  te  iubesc  cade  \n",
            "în  mare;  Şi  mumă-mea  \n",
            "\n",
            "el  sânul  se-ncheagă;  ochii  \n",
            "linişte  nici  Să  Mă  \n",
            "să  fii  pus  copilaş  \n",
            "În  mări  În  ca  \n",
            "\n",
            "de  moarte.  mari  durere,  \n",
            "înger  se  ştii,  şi-n  \n",
            "bine  ce  Uimirea-n  Şi-ncetişor  \n",
            "Dorinţele-i  sărutare,  ocean  Corăbii  \n",
            "\n",
            "doar  Un  soare  Cătălin?  \n",
            "luminii  cer  orişicare  cad,  \n",
            "Ca  S-anin  pripas,  se  \n",
            "pleacă-n  orişicare  calea  -  \n",
            "\n",
            "Tu-mi  lăudat  apară.  cu  \n",
            "el  fără  giulgi  durează-n  \n",
            "vecinicii,  Şi  din  stele.  \n",
            "asculta  Nu  geana  se  \n",
            "\n",
            "apucă:  Hyperion,  moare,  Un  \n",
            "soare  Când  el  mai  \n",
            "Luceşte  trestii.  Un  chip  \n",
            "vorbă  frumos  şi-l  Hyperion,  \n",
            "\n",
            "meu  -  Tu-mi  socoţi  \n",
            "să-mi  vecinicii,  Să  a  \n",
            "chaosului  sân,  moale,  -  \n",
            "Tu-mi  A-mpărătesii  a  se  "
          ]
        }
      ]
    }
  ]
}