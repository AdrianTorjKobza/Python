{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_GXjHffmz_Id"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.nn.utils import clip_grad_norm_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM = Long-Short-Term Memory is a type of recurrent neural network (RNN).\n",
        "\n",
        "embed_size = 128 # The numbner of values we are using to represent a word. This also represents the number of input features to LSTM.\n",
        "hidden_size = 1024 # The number of hidden units.\n",
        "num_layers = 2 # The number of layers.\n",
        "num_epochs = 18 # The number of epochs / complete iterations through a given dataset.\n",
        "words_batch_size = 20 # The number of training samples / words.\n",
        "time_steps = 16 # The number of previous words that will be looked at.\n",
        "learning_rate = 0.002 # The learning rate.\n",
        "num_words = 160 # The number of new words we want to generate.\n",
        "\n",
        "file_path = 'luceafarul.txt'"
      ],
      "metadata": {
        "id": "yWbd7DC9vAnT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to capture all the words from the poem, into a dictionary.\n",
        "class WordDictionary(object):\n",
        "  def __init__(self):\n",
        "    self.word_to_idx = {}\n",
        "    self.idx_to_word = {}\n",
        "    self.idx = 0\n",
        "\n",
        "  def add_word(self, word):\n",
        "    # Check if the word is not already in the dictionary.\n",
        "    if word not in self.word_to_idx:\n",
        "      self.word_to_idx[word] = self.idx # Map the word to an index.\n",
        "      self.idx_to_word[self.idx] = word # Map the index to a word.\n",
        "      self.idx = self.idx + 1\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.word_to_idx)"
      ],
      "metadata": {
        "id": "rZntzM5yjkoF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to process the words from file.\n",
        "class WordProcess(object):\n",
        "  def __init__(self):\n",
        "    self.dictionary = WordDictionary()\n",
        "\n",
        "  # Parse the file and capture all the words.\n",
        "  def get_data(self, path, batch_size):\n",
        "    with open(path, 'r') as f:\n",
        "      tokens = 0\n",
        "\n",
        "      # Loop through each line of the file.\n",
        "      for line in f:\n",
        "        words = line.split() # Split each line and create a list of words.\n",
        "        tokens = tokens + len(words) # Increment the tokens by the total number of words from each line.\n",
        "        \n",
        "        # Loop through the list of words.\n",
        "        for word in words:\n",
        "          self.dictionary.add_word(word) # Add each word into the word dictionary.\n",
        "\n",
        "    # Create a 1D tensor that captures all the words indexes.\n",
        "    idx_tensor = torch.LongTensor(tokens)\n",
        "    index = 0\n",
        "\n",
        "    with open(path, 'r') as f:\n",
        "      for line in f:\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "          idx_tensor[index] = self.dictionary.word_to_idx[word] # Map the tensor index to word index.\n",
        "          index = index + 1\n",
        "  \n",
        "    num_batches = idx_tensor.shape[0] // batch_size # We use // to ignore the remainder when we devide.\n",
        "    idx_tensor = idx_tensor[:num_batches * batch_size] # Trim the tensor to fit the batch size of 20.\n",
        "    idx_tensor = idx_tensor.view(batch_size, -1) # Reshape the tensor to have the number of rows 20 (batch size).\n",
        "    return idx_tensor"
      ],
      "metadata": {
        "id": "2uvrtB5Tjkqg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = WordProcess()\n",
        "idx_tensor = data.get_data(file_path, words_batch_size) # Load the file content into tensor.\n",
        "print (idx_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teRML5qcu-7r",
        "outputId": "4023ba50-2f27-419a-c0a4-0b02d2adcc13"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 88])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(data.dictionary)\n",
        "print (\"We have {} unique words.\".format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQxvbgXb0AaI",
        "outputId": "8536423a-0685-4cff-a013-71cf9d879204"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 982 unique words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = idx_tensor.shape[1] // time_steps # The number of batches required to complete the dataset (112 / 16).\n",
        "print (num_batches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtGmqXoe021-",
        "outputId": "35d66544-b2e4-4e0d-df22-30a747514da9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class that will generate the new text.\n",
        "class GenerateText(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "    super(GenerateText, self).__init__() # Use super function to inherit from nn.Module.\n",
        "    self.embed = nn.Embedding(vocab_size, embed_size) # Create the embedding layer that maps the words to features.\n",
        "    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first = True) # Create the LSTM layer.\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size) # Create the linear layer.\n",
        "\n",
        "  # Propagate the data through the neural network (x = inputs, h = hidden states).\n",
        "  def forward(self, x, h):\n",
        "    x = self.embed(x)\n",
        "    out, (h, c) = self.lstm(x, h)\n",
        "    out = out.reshape(out.size(0) * out.size(1), out.size(2))\n",
        "    out = self.linear(out)\n",
        "    return out, (h, c)"
      ],
      "metadata": {
        "id": "6-9gfevCu--8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenerateText(vocab_size, embed_size, hidden_size, num_layers) # Define the model.\n",
        "loss_func = nn.CrossEntropyLoss() # Define the loss function.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) # Define the optimization algorithm."
      ],
      "metadata": {
        "id": "lxgaOpK85fMe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using the mini-batch gradient descent.\n",
        "for epoch in range(num_epochs):\n",
        "  # Initialize the cell and hidden states.\n",
        "  states = (torch.zeros(num_layers, words_batch_size, hidden_size),\n",
        "            torch.zeros(num_layers, words_batch_size, hidden_size))\n",
        "  \n",
        "  # Iterate over the training dataset.\n",
        "  for i in range(0, idx_tensor.size(1) - time_steps, time_steps):\n",
        "    inputs = idx_tensor[:, i:i + time_steps]\n",
        "    targets = idx_tensor[:, (i + 1):(i + 1) + time_steps]\n",
        "    outputs,_ = model(inputs, states) # Feed the inputs and the cell & hidden states into the RNN model. \n",
        "    loss = loss_func(outputs, targets.reshape(-1)) # Calculate the loss between the predicted outputs and targets.\n",
        "\n",
        "    # Perform back-propagation and weight updates.\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    clip_grad_norm_(model.parameters(), 0.5) # Clip the gradient to avoid the exploding gradient problem.\n",
        "    optimizer.step() # Update the model parameters, using the step method.\n",
        "\n",
        "    step = (i + 1) // time_steps\n",
        "    \n",
        "    if step % 100 == 0:\n",
        "      print ('Epoch: {}/{}, Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NetV4C_Djksx",
        "outputId": "00c68cdc-2fa5-4a33-e610-9e5ebda999e0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/18, Loss: 6.8906\n",
            "Epoch: 2/18, Loss: 6.3861\n",
            "Epoch: 3/18, Loss: 6.1676\n",
            "Epoch: 4/18, Loss: 5.9043\n",
            "Epoch: 5/18, Loss: 5.1746\n",
            "Epoch: 6/18, Loss: 4.1960\n",
            "Epoch: 7/18, Loss: 3.1624\n",
            "Epoch: 8/18, Loss: 2.4553\n",
            "Epoch: 9/18, Loss: 1.8431\n",
            "Epoch: 10/18, Loss: 1.4414\n",
            "Epoch: 11/18, Loss: 1.1963\n",
            "Epoch: 12/18, Loss: 1.0304\n",
            "Epoch: 13/18, Loss: 0.7954\n",
            "Epoch: 14/18, Loss: 0.7101\n",
            "Epoch: 15/18, Loss: 0.5318\n",
            "Epoch: 16/18, Loss: 0.3366\n",
            "Epoch: 17/18, Loss: 0.2277\n",
            "Epoch: 18/18, Loss: 0.1231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model.\n",
        "words_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Initialize the cell and hidden states.\n",
        "  state = (torch.zeros(num_layers, 1, hidden_size),\n",
        "           torch.zeros(num_layers, 1, hidden_size))\n",
        "  \n",
        "  input = torch.randint(0, vocab_size, (1,)).long().unsqueeze(1) # Randomly select one word id and convert it to shape (1, 1)\n",
        "\n",
        "  for i in range(num_words):\n",
        "    output,_ = model(input, state) # Feed the input and the cell & hidden states into the RNN model. \n",
        "\n",
        "    sample_id = output.exp() # Sample a word id from the exponential output. \n",
        "    word_id = torch.multinomial(sample_id, num_samples = 1).item()\n",
        "    input.fill_(word_id) # Replace the input with sampled word id for the next time step.\n",
        "\n",
        "    word = data.dictionary.idx_to_word[word_id] # Capture the word associated with the id.\n",
        "    words_list.append(word) # Append the new word to the list of words."
      ],
      "metadata": {
        "id": "gA7uWZigPDGY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to create the usual poem format (4 words x 4 lines).\n",
        "def print_new_words(w_list):\n",
        "  for i, w in enumerate(w_list):\n",
        "    if i % 4 == 0 and i != 0:\n",
        "      print('\\n', end = '') # Add a new line every 4 words.\n",
        "      if i % 16 == 0:\n",
        "        print ('\\n', end = '') # Add an empty row, every 4 lines.\n",
        "\n",
        "    print (w + ' ', end = '')"
      ],
      "metadata": {
        "id": "7ZigDteESBE0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_new_words(words_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2LwBDT1VyDg",
        "outputId": "6a743c61-cf40-4b6e-dd24-a57d3adf42f9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "îmi mă paşte, - \n",
            "Tu-mi o acu-i vrei, \n",
            "legat, Ci păr Dar \n",
            "ochii îmbătată Dorinţele-i vom \n",
            "\n",
            "trecură încrede: mă pas \n",
            "cu el în mare; \n",
            "o umerele văi trupu-i \n",
            "ochii ochii mari Căci \n",
            "\n",
            "În de-o pat să-mi \n",
            "Pe marginea sus, a \n",
            "S-a timp, de luceferi. \n",
            "Vrei ce-mi ceri, zare \n",
            "\n",
            "nu-i Răsai s-or luceafăr \n",
            "rază, linişte capul vin'! \n",
            "mână şi-o şi-n Un \n",
            "Pe celei Marmoreele îmi \n",
            "\n",
            "mai curmă, vom copil \n",
            "Şi-i calea recile-i oglindă, \n",
            "pas albă aş copilaş \n",
            "fugi unde-ajunge mort, mor \n",
            "\n",
            "nici vom coate-şi florile-argintii \n",
            "Ce-ţi tine oare, vin, \n",
            "chip Dar ochii din \n",
            "cer a se naşte. \n",
            "\n",
            "cununi luceafăr, chip de \n",
            "Şedeau sus, calea idealuri \n",
            "se Şi mumă-mea vecinicului \n",
            "străvezii E ca scânteie-n \n",
            "\n",
            "paşte, Dar se culce \n",
            "Iubito, Fii sau prins \n",
            "ziua Cum potrivi să-mi \n",
            "fii dus. stele. pe-o \n",
            "\n",
            "Norocu-mi venii Pe creştetele-a \n",
            "un fulger - Tu-mi \n",
            "duce... luceafărul de luceferi. \n",
            "dedesubt, lasă, Şi-i nu-l \n",
            "\n",
            "e inim-o O, vin'! \n",
            "mânia, acu-i mormânt, dulce-al \n",
            "sus Uimirea-n senine. alunge, \n",
            "fii lângă Cere-mi senine. "
          ]
        }
      ]
    }
  ]
}