{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "9PXCGLWAWr1o"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the parameters.\n",
        "learning_rate = 3e-4 # Set the learning rate.\n",
        "batch_size = 32 # The number of training examples that will be used in each iteration of the loop.\n",
        "block_size = 64 # Set the total numbers of characters we are going to use for prediction.\n",
        "max_iters = 5000\n",
        "eval_interval = 1000\n",
        "eval_iters = 200\n",
        "n_embd = 64 # Number of embedded dimensions.\n",
        "n_head = 4 # Number of heads.\n",
        "n_layer = 4 # Number of layers.\n",
        "dropout = 0.2 # Set the probability for dropping outputs / setting outputs to zero.\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Run the model on GPU if available, otherwise on CPU.\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR29ZFjCtjYk",
        "outputId": "d0f25b56-cb7b-44f2-9e44-c511e705469c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6fcc5299b0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the input dataset, the famous \"Luceafarul\" poem by Mihai Eminescu.\n",
        "!wget https://raw.githubusercontent.com/AdrianTorjKobza/Python/main/AI_ChatGPT_Mini/luceafarul.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d-lO4rlYL4A",
        "outputId": "92b50824-b97d-42cd-ea51-7e8fcc8d9804"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-01 11:17:19--  https://raw.githubusercontent.com/AdrianTorjKobza/Python/main/AI_ChatGPT_Mini/luceafarul.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11052 (11K) [text/plain]\n",
            "Saving to: ‘luceafarul.txt’\n",
            "\n",
            "\rluceafarul.txt        0%[                    ]       0  --.-KB/s               \rluceafarul.txt      100%[===================>]  10.79K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-03-01 11:17:19 (95.6 MB/s) - ‘luceafarul.txt’ saved [11052/11052]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open and read the input file.\n",
        "with open('luceafarul.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "FoDw7rWE6F_R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all the unique characters from text.\n",
        "chars = set(text) # Get a set off all unique characters from text.\n",
        "chars = list(chars) # Create a list of all the unique characters.\n",
        "chars = sorted(chars) # Sort the characters in ascending order.\n",
        "\n",
        "print ('List of all characters: ', ''.join(chars)) # Print all the unique characters\n",
        "\n",
        "vocab_size = len(chars)\n",
        "print ('\\nVocabulary size is', vocab_size, 'unique characters.') # Print the total numbers of characters."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDepAxgd6s3x",
        "outputId": "65a3f6c7-46fb-453e-ccd8-2daf601ea34f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of all characters:  \n",
            " !',-.:;?ABCDEFGHIJLMNOPRSTUVabcdefghijlmnoprstuvyzÎâîăŞşŢţ\n",
            "\n",
            "Vocabulary size is 60 unique characters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the characters of the input text. Each character will be tokenized, thus we will have 65 tokens.\n",
        "# Mapp the list of characters to integers.\n",
        "\n",
        "# Create empty dictionaries.\n",
        "stoi = {}\n",
        "itos = {}\n",
        "\n",
        "# Iterate over the list of chars and return both the index and character.\n",
        "for i, ch in enumerate(chars):\n",
        "  stoi.update({ch:i}) # Update the dictionary and mapp each character to it's corresponding index 'i'.\n",
        "  itos.update({i:ch}) # Update the dictionary and mapp each index to it's corresponding character 'ch'\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s] # Return the list of indices corresponding to each character from string 's'.\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # Return the list of characters corresponding to each index from list of indices 'l'. \n",
        "\n",
        "print ('The list of integers mapped to a character:\\n', encode(chars))\n",
        "print (decode(encode(chars)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sB6Yi8a9uHn",
        "outputId": "9554c602-db4b-4497-f29a-42203c01b282"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The list of integers mapped to a character:\n",
            " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "\n",
            " !',-.:;?ABCDEFGHIJLMNOPRSTUVabcdefghijlmnoprstuvyzÎâîăŞşŢţ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the entire text and store it into a torch tensor.\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "\n",
        "print (data.shape, data.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBVo37QJEdkY",
        "outputId": "2abcf521-a4a2-4c63-cf55-b4dce7e5f36a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([9830]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation datasets, to avoid overfitting.\n",
        "n = int(0.9 * len(data)) # Set the first 90% of the data, to be part of the training dataset. The remaining 10% is part of the validation dataset.\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "LwTc_5jKGJ9K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a batch of inputs x and targets y.\n",
        "def get_batch(split):\n",
        "  if split == 'train':\n",
        "    data = train_data\n",
        "  else:\n",
        "    data = val_data\n",
        "\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,)) # Generate a batch of random starting indices.\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix]) # Create a tensor by stacking blocks of data.\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # Create a tensor by stacking blocks of data, offset by 1.\n",
        "  x, y = x.to(device), y.to(device)\n",
        "\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "hpbMcQ5bPEBj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define decorator that temporarily disables the gradient computation.\n",
        "# The operations performed inside the decorated block will not have their gradients tracked.\n",
        "@torch.no_grad()\n",
        "\n",
        "# Get the average loss over multiple batches.\n",
        "# Evaluate the loss when iterating over the training and validation datasets.\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval() # Set the model to evaluation phase.\n",
        "\n",
        "    # Iterate over the training and validation datasets.\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters) # Create a tensor with all elements initialized to zero.\n",
        "        \n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split) # Get a batch of data.\n",
        "            logits, loss = model(X, Y) # Evaluate the loss on the batch of data.\n",
        "            losses[k] = loss.item() # Store the loss value.\n",
        "        out[split] = losses.mean() # Compute the mean loss.\n",
        "\n",
        "    model.train() # Set the model back to training mode.\n",
        "    return out # Return the dictionary, containing the mean loss for each data split."
      ],
      "metadata": {
        "id": "lMbKmk2GywL3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Head class (subclass of nn.Module) for self-attention.\n",
        "class Head(nn.Module):\n",
        "\n",
        "  # Apply liniar projections to all the nodes.\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False) # Mapp each element of the input tensor to a vector of dimension 'head_size', that represents the associated key.\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False) # Mapp each element of the input tensor to a vector of dimension 'head_size', that represents the associated query.\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False) # Mapp each element of the input tensor to a vector of dimension 'head_size', that represents the associated value.\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # Create the lower triangle matrix, where we have 1s in the triangle, and 0s for the rest.\n",
        "    self.dropout = nn.Dropout(dropout) # Use Dropout to prevent overfitting, by randomly setting outputs to zero.\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape # Unpack the shape of the input tensor into variables B, T, C.\n",
        "    k = self.key(x) # Format: (B, T, C). Generate a tensor that contains the keys associated with each element in 'x'.\n",
        "    q = self.query(x) # Format: (B, T, C). Generate a tensor that contains the queries associated with each element in 'x'.\n",
        "\n",
        "    # Calculate the attention scores.\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) => (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Format: (B, T, T). We make sure future doesn't communicate with the past.\n",
        "    wei = F.softmax(wei, dim=-1) # Format: (B, T, T)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    # Aggregate the values.\n",
        "    v = self.value(x) # Format: (B,T,C)\n",
        "    out = wei @ v # (B, T, T) @ (B, T, C) => (B, T, C)\n",
        "    return out"
      ],
      "metadata": {
        "id": "1IgHL-y8MkQa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MultiHead class (subclass of nn.Module) for multi head self-attention / multiple self-attention in parallel.\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # Create multiple heads.\n",
        "    self.proj = nn.Linear(n_embd, n_embd) # Concatenate the multiple attention heads output into a single vector.\n",
        "    self.dropout = nn.Dropout(dropout) # Use Dropout to prevent overfitting, by randomly setting outputs to zero.\n",
        "\n",
        "  # Run in parallel, all the one-head self-attention, and concatenate the outputs.\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.proj(out)\n",
        "    out = self.dropout(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "ldIfD8lqaxXx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for a feed forward neural network.\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "NJOeTokndrQu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replicate the transformers architecture, without the cross-attention portion.\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size) # Perform communication.\n",
        "        self.ffwd = FeedForward(n_embd) # Perform computation.\n",
        "\n",
        "        # Perform layer normalization.\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    # Apply layer normalization before it goes to self-attention and feed forward.\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "LfXPzTPXfiZr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigram Language Model (subclass of nn.Module) is a neural network that predicts the probability of the next word, given the previous word.\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # Store embeddings of a fixed dictionary and size into a lookup table.\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd) # Capture the information about the position of each token in the sequence.\n",
        "    \n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head = n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    \n",
        "    # Mapp the output of the embedding table to a probability distribution.\n",
        "    # The linear transformation layer applies a matrix multiplication and bias addition, followed by softmax activation function.\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  # 'inputs' is a tensor of indices representing the previous words and 'targets' is a tensor of indices representing the next word.\n",
        "  def forward(self, inputs, targets = None):\n",
        "    B, T = inputs.shape # Unpack the shape of the input tensor into variables B, T.\n",
        "\n",
        "    # Return the corresponding logits (unnormalized score) for each token in the vocabulary.\n",
        "    tok_emb = self.token_embedding_table(inputs) # Format (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device = device)) # Format: (T, C)\n",
        "    x = tok_emb + pos_emb # Combine the token embeddings and the positional embeddings. (B, T, C)\n",
        "    x = self.blocks(x) # Format: (B, T, C)\n",
        "    logits = self.lm_head(x) # Format: (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape # Unpack the logits into Batch, Time, Channels.\n",
        "      logits = logits.view(B*T, C) # Reshape logits into a two-dimensional tensor.\n",
        "      targets = targets.view(B*T) # Reshape targets into a one-dimenstional tensor.\n",
        "      loss = F.cross_entropy(logits, targets) # Compute the loss, to understand how well we are predicting the next character.\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  # Generate the model; the sequence of tokens.\n",
        "  # Take \"inputs\" as (Batch, Time) and generate (Batch, Time + 1, +2, +3, ... 'max_new_tokens')\n",
        "  def generate(self, inputs, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      inputs_cond = inputs[:, -block_size:] # Crop 'inputs' to the last block_size tokens.\n",
        "      logits, loss = self(inputs_cond) # Call the forward method.\n",
        "      logits = logits[:, -1, :] # Select the logits corresponding to the last token in the sequence. Format: (Batch, Channels).\n",
        "      probs = F.softmax(logits, dim=-1) # Apply softmax to get the probabilities.\n",
        "      inputs_next = torch.multinomial(probs, num_samples=1) # Sample the next token. Format: (Batch, 1)\n",
        "      inputs = torch.cat((inputs, inputs_next), dim=1) # Append the next token to the sequence. Format: (Batch, Time + 1)\n",
        "\n",
        "    return inputs\n",
        "\n",
        "model = BigramLanguageModel() # Create the model.\n",
        "m = model.to(device)"
      ],
      "metadata": {
        "id": "Y-avTxggYFlE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.zeros((1, 1), dtype = torch.long, device = device) # Create a 1 by 1 tensor, initiliazed to all zeroes.\n",
        "\n",
        "# Generate and print 100 new tokens / characters.\n",
        "# The new generated content will be junk, since the model is not yet trained.\n",
        "print ('New content:\\n', decode(m.generate(inputs, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPvBU3k-wQ_1",
        "outputId": "a62a224c-787c-479e-c078-1d256baf83ce"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New content:\n",
            " \n",
            "şOiŞnfŢASBzevvTAcşBâHŞuveN!h.bFggHcu-îAhdlIinU:l,n'MşşDssaJCrl-\n",
            "zJmzeŢ!FEcIPoş'aŞFţŞNRmţA!hzbF\n",
            "RePSo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PyTorch optimizer object.\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "bq7bl8GcIck3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model.\n",
        "for iter in range(max_iters):\n",
        "  # Evaluate and print the loss on training and validation datasets.\n",
        "  if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"Step {iter}: Training loss is {losses['train']:.4f}, Validation loss is {losses['val']:.4f}\")\n",
        "\n",
        "  xb, xy = get_batch('train') # Sample a batch of data.\n",
        "\n",
        "  # Evaluate the loss.\n",
        "  logits, loss = model(xb, xy)\n",
        "  optimizer.zero_grad(set_to_none = True) # Set all gradients to zero, from the previous step.\n",
        "  loss.backward() # Get the gradients from all the parameters, using backpropagation / chain rule.\n",
        "  optimizer.step() # Use the gradients to update the parameters; to optimize the model and minimize the loss."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWIrWg99IyGA",
        "outputId": "6422d763-e9ac-45d6-aa16-fdb5bed16838"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Training loss is 4.3961, Validation loss is 4.4125\n",
            "Step 1000: Training loss is 2.2384, Validation loss is 2.3877\n",
            "Step 2000: Training loss is 1.9960, Validation loss is 2.2646\n",
            "Step 3000: Training loss is 1.7462, Validation loss is 2.1821\n",
            "Step 4000: Training loss is 1.5055, Validation loss is 2.1461\n",
            "Step 4999: Training loss is 1.2820, Validation loss is 2.1371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.zeros((1, 1), dtype = torch.long, device = device) # Create a 1 by 1 tensor, initiliazed to all zeroes.\n",
        "\n",
        "# Generate and print 1000 new tokens / characters.\n",
        "print ('New content:\\n', decode(m.generate(inputs, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "id": "Uv4lR24qLQUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc1eaa5-9dd8-40e7-aad1-50d85048b442"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New content:\n",
            " \n",
            "Drumin chefrăteri\n",
            "Tu spirătele ca mândrâng\n",
            "Und fru-aşu-i min suringRămâng\n",
            "Să îndrăcu fin mândrure.\n",
            "Tel tunde-un ar zie copr crag\n",
            "\n",
            "Mim ca braştea cizreşte -\n",
            "U rumon marde şi scă şi olui men,\n",
            "Ca to umbi a năcainori,\n",
            "Şi din umplea fevăciiş\n",
            "\n",
            "Cărei ste line acândrita-n l-n aung,\n",
            "Cu- eacundoi upăr mă loan,\n",
            "Ca tân'!eli ajore tă cupăsat,\n",
            "Eu văi mă atu pl cecum nesple cu doc-n m-o ntul\n",
            "Şi vezi cu cumi se re-\n",
            "Şi atrecămbii nicu viaţa;\n",
            "\n",
            "\n",
            "ânt vin', mrizineaţi, leci\n",
            "Cătoi tu mbrastele doţi,\n",
            "Şi didoin de-o calu-nil tir?\n",
            "Stinţiitos, fintrăul boatesci\n",
            "Ţe de moar iptie cunu cu coare,\n",
            "\n",
            "Cătri Dar pas vit mă-n moar,\n",
            "Băe-l a în aplără.\n",
            "\n",
            "Ea cumNu alu priivis peri\n",
            "\n",
            "Răstrecine a muritos, tă,\n",
            "Căgerde zinele-i lece an\n",
            "\n",
            "Şi iu pa ununzi-mi cese rus,\n",
            "Iar şii treu, în ndroc brapă\n",
            "Să uprând duri;\n",
            "\n",
            "Şi n', pe-arăzil ec de-\n",
            "Şi pene-ndrtre-oceafod vinţi\n",
            "\n",
            "Rătând sine-n astă-şe,\n",
            "Să în fapoc lumine;\n",
            "Lucucecândar pe.\n",
            "\n",
            "În copţile goate, sus de făr\n",
            "Dar mar c-a'n s'fi-oscă-n dere;\n",
            "\n",
            "Tu-os, un'!tos\n",
            "De nu nece tagiar de-au nundar\n"
          ]
        }
      ]
    }
  ]
}
